{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZbR2ZT9G2Fw"
      },
      "outputs": [],
      "source": [
        "#                                                                            \n",
        "#    Copyright 2022\n",
        "#    Alexander Belyi <alexander.belyi@gmail.com>,\n",
        "#    Stanislav Sobolevsky <sobolevsky@nyu.edu>                                               \n",
        "#                                                                            \n",
        "#    This file contains the source code of the GNNS algorithm and its evaluation.\n",
        "#\n",
        "#    This program is free software: you can redistribute it and/or modify\n",
        "#    it under the terms of the GNU General Public License as published by\n",
        "#    the Free Software Foundation, either version 3 of the License, or\n",
        "#    (at your option) any later version.\n",
        "#\n",
        "#    This program is distributed in the hope that it will be useful,\n",
        "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "#    GNU General Public License for more details.\n",
        "#\n",
        "#    You should have received a copy of the GNU General Public License\n",
        "#    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CutCrYZ0BzZM",
        "outputId": "41ccd22e-9a19-43bc-8d95-068a963161f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GNNS'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 30 (delta 5), reused 26 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Alexander-Belyi/GNNS.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h2qzaJTe9B_",
        "outputId": "cf38b7a3-09ed-4e62-f9ca-546c71de5086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycombo\n",
            "  Downloading pycombo-0.1.7.tar.gz (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 31.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11<3.0.0,>=2.6.1\n",
            "  Downloading pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<2.0,>=1.0\n",
            "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->pycombo) (3.8.0)\n",
            "Building wheels for collected packages: pycombo\n",
            "  Building wheel for pycombo (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycombo: filename=pycombo-0.1.7-cp37-cp37m-manylinux_2_27_x86_64.whl size=98830 sha256=9e19beca46226f3c9742a00b868b377976ed462204d0dc244d4b1c47ed62f386\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/52/18/4c1b80cd45c091e2c1ea442729343ac984dc66b3a678e2c251\n",
            "Successfully built pycombo\n",
            "Installing collected packages: pybind11, importlib-metadata, pycombo\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.4\n",
            "    Uninstalling importlib-metadata-4.11.4:\n",
            "      Successfully uninstalled importlib-metadata-4.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-1.7.0 pybind11-2.9.2 pycombo-0.1.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting leidenalg\n",
            "  Downloading leidenalg-0.8.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 33.9 MB/s \n",
            "\u001b[?25hCollecting igraph<0.10,>=0.9.0\n",
            "  Downloading igraph-0.9.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 56.7 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, leidenalg\n",
            "Successfully installed igraph-0.9.11 leidenalg-0.8.10 texttable-1.6.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cdlib\n",
            "  Downloading cdlib-0.2.6-py3-none-any.whl (228 kB)\n",
            "\u001b[K     |████████████████████████████████| 228 kB 32.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from cdlib) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cdlib) (1.21.6)\n",
            "Collecting demon\n",
            "  Downloading demon-2.0.6-py3-none-any.whl (7.3 kB)\n",
            "Collecting markov-clustering\n",
            "  Downloading markov_clustering-0.0.6.dev0-py3-none-any.whl (6.3 kB)\n",
            "Collecting angel-cd\n",
            "  Downloading angel_cd-1.0.3-py3-none-any.whl (10 kB)\n",
            "Collecting pulp\n",
            "  Downloading PuLP-2.6.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from cdlib) (0.29.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cdlib) (4.64.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from cdlib) (2.6.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.7/dist-packages (from cdlib) (1.6.0)\n",
            "Collecting python-igraph\n",
            "  Downloading python-igraph-0.9.11.tar.gz (9.5 kB)\n",
            "Collecting bimlpa\n",
            "  Downloading bimlpa-0.1.2-py3-none-any.whl (7.0 kB)\n",
            "Collecting pyclustering\n",
            "  Downloading pyclustering-0.10.1.2.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 54.1 MB/s \n",
            "\u001b[?25hCollecting dynetx\n",
            "  Downloading dynetx-0.3.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cdlib) (1.4.1)\n",
            "Requirement already satisfied: python-louvain>=0.16 in /usr/local/lib/python3.7/dist-packages (from cdlib) (0.16)\n",
            "Collecting eva-lcd\n",
            "  Downloading eva_lcd-0.1.1-py3-none-any.whl (9.2 kB)\n",
            "Collecting nf1\n",
            "  Downloading nf1-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from cdlib) (1.3.5)\n",
            "Collecting chinese-whispers\n",
            "  Downloading chinese_whispers-0.8.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting thresholdclustering\n",
            "  Downloading thresholdclustering-1.1-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from cdlib) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from cdlib) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from cdlib) (0.11.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from dynetx->cdlib) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cdlib) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cdlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cdlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cdlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->cdlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->cdlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->cdlib) (2022.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch->cdlib) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pooch->cdlib) (21.3)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch->cdlib) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch->cdlib) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch->cdlib) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch->cdlib) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch->cdlib) (3.0.4)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering->cdlib) (7.1.2)\n",
            "Requirement already satisfied: igraph==0.9.11 in /usr/local/lib/python3.7/dist-packages (from python-igraph->cdlib) (0.9.11)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from igraph==0.9.11->python-igraph->cdlib) (1.6.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->cdlib) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->cdlib) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->cdlib) (1.1.0)\n",
            "Building wheels for collected packages: pyclustering, python-igraph, python-Levenshtein\n",
            "  Building wheel for pyclustering (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyclustering: filename=pyclustering-0.10.1.2-py3-none-any.whl size=2395122 sha256=cf4c8d9a8280e7e2057d00409d59d182c28c658395b2423497bf8c557f519d18\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/87/6b/1e0568b5ba9dc6518a25338bae90bd8392f35206bb90bb10f1\n",
            "  Building wheel for python-igraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-igraph: filename=python_igraph-0.9.11-py3-none-any.whl size=9074 sha256=32466fca4fa49b0f07960bba0b9d34e5be37dbb3f04d1d78969c69035703f8cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/e4/0c/0c83f70bd0b99ce8aea47a21f8e52502169e7dd17808d12f30\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149870 sha256=bd1d2c69888032301f6f2bb1f68d2e4fce753b9bb18128cd5125621820e36c9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built pyclustering python-igraph python-Levenshtein\n",
            "Installing collected packages: python-igraph, thresholdclustering, python-Levenshtein, pyclustering, pulp, nf1, markov-clustering, eva-lcd, dynetx, demon, chinese-whispers, bimlpa, angel-cd, cdlib\n",
            "Successfully installed angel-cd-1.0.3 bimlpa-0.1.2 cdlib-0.2.6 chinese-whispers-0.8.0 demon-2.0.6 dynetx-0.3.1 eva-lcd-0.1.1 markov-clustering-0.0.6.dev0 nf1-0.0.4 pulp-2.6.0 pyclustering-0.10.1.2 python-Levenshtein-0.12.2 python-igraph-0.9.11 thresholdclustering-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pycombo\n",
        "!pip install leidenalg\n",
        "!pip install cdlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DWbhMGGfHIV",
        "outputId": "9de3847d-8e63-4913-ba41-837643b5e804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: to be able to use all crisp methods, you need to install some additional packages:  {'karateclub', 'graph_tool', 'wurlitzer', 'infomap'}\n",
            "Note: to be able to use all overlapping methods, you need to install some additional packages:  {'karateclub', 'ASLPAw'}\n",
            "Note: to be able to use all bipartite methods, you need to install some additional packages:  {'wurlitzer', 'infomap'}\n"
          ]
        }
      ],
      "source": [
        "import bz2\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy\n",
        "import networkx as nx\n",
        "import igraph as iGraph\n",
        "import leidenalg\n",
        "from pycombo import pyCombo\n",
        "from cdlib import algorithms, NodeClustering\n",
        "from multiprocessing import Pool\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx09T-zTGeB8",
        "outputId": "85e8cb2e-e132-4e1d-f859-0404503c0912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor: tensor([1., 2., 3., 4.])\n",
            "Tensor device: cpu\n",
            "CUDA GPU: True\n",
            "tensor([1., 2., 3., 4.], device='cuda:0')\n",
            "Tensor device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# create a tensor\n",
        "x = torch.tensor([1.0,2.0,3.0,4.0])\n",
        "print(\"Tensor:\", x)\n",
        "# check tensor device (cpu/cuda)\n",
        "print(\"Tensor device:\", x.device)\n",
        "# Move tensor from CPU to GPU\n",
        "# check CUDA GPU is available or not\n",
        "print(\"CUDA GPU:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "   x=x.to(\"cuda\")\n",
        "print(x)\n",
        "# now check the tensor device\n",
        "print(\"Tensor device:\", x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fBnYIimzh1im"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Engine:\n",
        "    def __init__(self, engine: str) -> None:\n",
        "        if engine == 'np':\n",
        "            self.array = self.array_np\n",
        "            self.sparse_array = self.sparse_array_np\n",
        "            self.diag = self.diag_np\n",
        "            self.sum = self.sum_np\n",
        "            self.sparse_sum = self.sparse_sum_np\n",
        "            self.mean = self.mean_np\n",
        "            self.max = self.max_np\n",
        "            self.argmax = self.argmax_np\n",
        "            self.eye = self.eye_np\n",
        "            self.ones = self.ones_np\n",
        "            self.zeros = self.zeros_np\n",
        "            self.abs = self.abs_np\n",
        "            self.exp = self.exp_np\n",
        "            self.concatenate = self.concatenate_np\n",
        "            self.reshape = self.reshape_np\n",
        "            self.transpose = self.transpose_np\n",
        "            self.tile = self.tile_np\n",
        "            self.matmul = self.matmul_np\n",
        "            self.random_uniform = self.random_uniform_np\n",
        "            self.to_sparse_csr = self.to_sparse_csr_np\n",
        "            self.cuda = self.identity_np\n",
        "            self.numpy = self.identity_np\n",
        "        elif engine == 'torch':\n",
        "            self.device = torch.device('cpu')\n",
        "            self.cuda_available = False\n",
        "            if torch.cuda.is_available():\n",
        "                self.cuda_available = True\n",
        "                self.device = torch.device(0)\n",
        "            self.array = self.array_torch\n",
        "            self.sparse_array = self.sparse_array_torch\n",
        "            self.diag = self.diag_torch\n",
        "            self.sum = self.sum_torch\n",
        "            self.sparse_sum = self.sparse_sum_torch\n",
        "            self.mean = self.mean_torch\n",
        "            self.max = self.max_torch\n",
        "            self.argmax = self.argmax_torch\n",
        "            self.eye = self.eye_torch\n",
        "            self.ones = self.ones_torch\n",
        "            self.zeros = self.zeros_torch\n",
        "            self.abs = self.abs_torch\n",
        "            self.exp = self.exp_torch\n",
        "            self.concatenate = self.concatenate_torch\n",
        "            self.reshape = self.reshape_torch\n",
        "            self.transpose = self.transpose_torch\n",
        "            self.tile = self.tile_torch\n",
        "            self.matmul = self.matmul_torch\n",
        "            self.random_uniform = self.random_uniform_torch\n",
        "            self.to_sparse_csr = self.to_sparse_csr_torch\n",
        "            self.cuda = self.cuda_torch\n",
        "            self.numpy = self.numpy_torch\n",
        "\n",
        "    def array_np(self, x, device=None):\n",
        "        return np.array(x)\n",
        "\n",
        "    def sparse_array_np(self, x, device=None):\n",
        "        #if type(x) == scipy.sparse.csr_array: # requires python >= 3.8\n",
        "        if type(x) == scipy.sparse.csr_matrix:\n",
        "            return x\n",
        "        #return scipy.sparse.csr_array(x)\n",
        "        return scipy.sparse.csr_matrix(x)\n",
        "\n",
        "    def diag_np(self, x):\n",
        "        return np.diag(x)\n",
        "\n",
        "    def sum_np(self, x, axis=None, keepdims=False):\n",
        "        return np.sum(x, axis=axis, keepdims=keepdims)\n",
        "\n",
        "    def sparse_sum_np(self, x, axis=None):\n",
        "        return np.array(np.sum(x, axis=axis))\n",
        "\n",
        "    def mean_np(self, x, axis=None, keepdims=False):\n",
        "        return np.mean(x, axis=axis, keepdims=keepdims)\n",
        "    \n",
        "    def max_np(self, x, axis=None, keepdims=False):\n",
        "        return np.max(x, axis=axis, keepdims=keepdims)\n",
        "\n",
        "    def argmax_np(self, x, axis=None): \n",
        "        return np.argmax(x, axis=axis)\n",
        "\n",
        "    def eye_np(self, size, dtype=None, device=None):\n",
        "        return np.eye(size, dtype=dtype)\n",
        "\n",
        "    def ones_np(self, size, dtype=None, device=None):\n",
        "        return np.ones(size, dtype=dtype)\n",
        "\n",
        "    def zeros_np(self, size, dtype=None, device=None):\n",
        "        return np.zeros(size, dtype=dtype)\n",
        "\n",
        "    def abs_np(self, x):\n",
        "        return np.abs(x)\n",
        "\n",
        "    def exp_np(self, x):\n",
        "        return np.exp(x)\n",
        "\n",
        "    def concatenate_np(self, x, axis=None):\n",
        "        return np.concatenate(x, axis=axis)\n",
        "    \n",
        "    def reshape_np(self, x, shape):\n",
        "        return np.reshape(x, shape)\n",
        "\n",
        "    def transpose_np(self, x, axis1, axis2):\n",
        "        return np.swapaxes(x, axis1, axis2)\n",
        "\n",
        "    def tile_np(self, x, shape):\n",
        "        return np.tile(x, shape)\n",
        "\n",
        "    def matmul_np(self, x, y):\n",
        "        return np.matmul(x, y)\n",
        "\n",
        "    def random_uniform_np(self, low, high, size, device=\"cpu\"):\n",
        "        return np.random.uniform(low=low, high=high, size=size)\n",
        "\n",
        "    def to_sparse_csr_np(self, x):\n",
        "        return x.tocsr()\n",
        "\n",
        "    def identity_np(self, x):\n",
        "        return x\n",
        "\n",
        "    ### torch\n",
        "    def array_torch(self, x, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.tensor(x, device=device)\n",
        "\n",
        "    def sparse_array_torch(self, x, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        #coo_x = scipy.sparse.coo_array(x) # requires python >=3.8\n",
        "        coo_x = scipy.sparse.coo_matrix(x)\n",
        "        return torch.sparse_coo_tensor(np.vstack((coo_x.row, coo_x.col)), coo_x.data, size=coo_x.shape, device=device)\n",
        "\n",
        "    def diag_torch(self, x):\n",
        "        return torch.diag(x)\n",
        "\n",
        "    def sum_torch(self, x, axis=None, keepdims=False):\n",
        "        return torch.sum(x, axis=axis, keepdims=keepdims)\n",
        "\n",
        "    def sparse_sum_torch(self, x, axis=None):\n",
        "        return torch.sparse.sum(x, dim=axis).to_dense()\n",
        "\n",
        "    def mean_torch(self, x, axis=None, keepdims=False):\n",
        "        return torch.mean(x, axis=axis, keepdims=keepdims)\n",
        "    \n",
        "    def max_torch(self, x, axis=None, keepdims=False):\n",
        "        if axis is None:\n",
        "            return torch.max(x, axis=axis, keepdims=keepdims)\n",
        "        else:\n",
        "            return torch.max(x, axis=axis, keepdims=keepdims).values\n",
        "\n",
        "    def argmax_torch(self, x, axis=None, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.argmax(x, axis=axis).to(device)\n",
        "\n",
        "    def eye_torch(self, size, dtype=None, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.eye(size, dtype=dtype, device=device)\n",
        "\n",
        "    def ones_torch(self, size, dtype=None, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.ones(size, dtype=dtype, device=device)\n",
        "\n",
        "    def zeros_torch(self, size, dtype=None, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.zeros(size, dtype=dtype, device=device)\n",
        "\n",
        "    def abs_torch(self, x):\n",
        "        return torch.abs(x)\n",
        "\n",
        "    def exp_torch(self, x):\n",
        "        return torch.exp(x)\n",
        "\n",
        "    def concatenate_torch(self, x, axis=0):\n",
        "        return torch.cat(x, dim=axis)\n",
        "    \n",
        "    def reshape_torch(self, x, shape):\n",
        "        return torch.reshape(x, shape)\n",
        "\n",
        "    def transpose_torch(self, x, dim0, dim1):\n",
        "        return torch.transpose(x, dim0, dim1)\n",
        "\n",
        "    def tile_torch(self, x, shape):\n",
        "        return torch.tile(x, shape)\n",
        "\n",
        "    def matmul_torch(self, x, y):\n",
        "        return torch.matmul(x, y)\n",
        "\n",
        "    def random_uniform_torch(self, low, high, size, dtype=None, device=\"cpu\"):\n",
        "        if not self.cuda_available:\n",
        "            device = \"cpu\"\n",
        "        return torch.zeros(size, dtype=dtype, device=device).uniform_(low, high)\n",
        "\n",
        "    def to_sparse_csr_torch(self, x):\n",
        "        return x.to_sparse_csr()\n",
        "\n",
        "    def cuda_torch(self, x):\n",
        "        if not self.cuda_available:\n",
        "            return x\n",
        "        return x.cuda()\n",
        "    \n",
        "    def numpy_torch(self, x):\n",
        "        return x.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C_FK0hhZfB4y"
      },
      "outputs": [],
      "source": [
        "def set_all_random_seeds(seed=1):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_modularity_matrix(G, symmetrize=True, loops_style=2, device=\"cpu\"):\n",
        "    '''build modularity matrix'''\n",
        "    A = nx.to_numpy_array(G)\n",
        "    A = eng.array(A, device)\n",
        "    if loops_style == 2 and not G.is_directed():\n",
        "        A += eng.diag(eng.diag(A))\n",
        "    elif loops_style == 0:\n",
        "        A -= eng.diag(eng.diag(A))\n",
        "    w_in = A.sum(axis=0, keepdims=True)\n",
        "    w_out = A.sum(axis=1, keepdims=True)\n",
        "    T = w_out.sum()\n",
        "    Q = A / T - w_out @ w_in / (T ** 2)\n",
        "    if symmetrize:\n",
        "        Q = (Q + Q.T) / 2\n",
        "    return Q\n",
        "\n",
        "def get_sparse_modularity_matrix(G, loops_style=2, device=\"cpu\"):\n",
        "    '''build sparse modularity matrix'''\n",
        "    #A = nx.to_scipy_sparse_array(G, format=\"coo\") #requires python >= 3.8\n",
        "    A = nx.to_scipy_sparse_matrix(G, dtype=float, format=\"coo\")\n",
        "    if loops_style == 2 and not G.is_directed():\n",
        "        A.setdiag(A.diagonal() * 2)\n",
        "    elif loops_style == 0:\n",
        "        A.setdiag(0)\n",
        "    Q_diag = eng.array(A.diagonal(), device)\n",
        "    A = eng.sparse_array(A, device)\n",
        "    w_in = eng.sparse_sum(A, axis=0).reshape((1, A.shape[0]))\n",
        "    w_out = eng.sparse_sum(A, axis=1).reshape((A.shape[0], 1))\n",
        "    T = w_out.sum()\n",
        "    w_in /= T\n",
        "    w_out /= T\n",
        "    Q = A / T\n",
        "    Q_diag /= T\n",
        "    Q = eng.to_sparse_csr(Q)\n",
        "    # Q -= w_out @ w_in\n",
        "    return Q, Q_diag, w_out, w_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BMt1naxFiHsV"
      },
      "outputs": [],
      "source": [
        "class GNNSModularityOptimizer():\n",
        "    def __init__(self, G, strip_diagonal=True, normalize_modularity=False,\n",
        "                 normalize_each_step=True, normalize_QC=True, use_sparse=False):\n",
        "        self.num_model_params = 2\n",
        "        self.net_size = len(G)\n",
        "        self.normalize_modularity = normalize_modularity\n",
        "        self.use_sparse = use_sparse\n",
        "        #strip diagonal elements of the modularity matrix when initializing the data\n",
        "        self.strip_diagonal = strip_diagonal\n",
        "        if self.use_sparse:\n",
        "            self.sparse_Q, self.Q_diag, self.w_out, self.w_in = get_sparse_modularity_matrix(G, device=\"cuda:0\")\n",
        "            self.Q_diag = eng.reshape(self.Q_diag, (1, self.net_size, 1))\n",
        "            self.Q_diag -= eng.reshape(self.w_out * eng.transpose(self.w_in, 0, 1), (1, self.net_size, 1))\n",
        "        else:\n",
        "            self.Q = get_modularity_matrix(G, symmetrize=True, device=\"cuda:0\")\n",
        "            self.Q_diag = eng.reshape(eng.diag(self.Q), (1, self.net_size, 1))\n",
        "        self.reshape_Q()\n",
        "        self.normalize_each_step = normalize_each_step\n",
        "        self.normalize_QC = normalize_QC\n",
        "\n",
        "    def reshape_Q(self):\n",
        "        if self.use_sparse:\n",
        "            self.w_out = self.w_out[None, :, :]\n",
        "            self.w_in = self.w_in[None, :, :]\n",
        "        else:\n",
        "            self.Q = self.Q.reshape((1, *self.Q.shape)) # add batch dimension\n",
        "            if self.normalize_modularity:\n",
        "                w = eng.sum(eng.abs(self.Q), axis=2, keepdims=True)\n",
        "                self.Q /= w + (w==0)\n",
        "\n",
        "    def reshape_model_params(self, params):\n",
        "        f0 = eng.cuda(-params[:, 0:1, None])\n",
        "        f1 = eng.cuda(params[:, 1:2, None])\n",
        "        f2 = 1.0 - f0 - f1\n",
        "        return f0, f1, f2\n",
        "\n",
        "    def discretize(self):\n",
        "        c = eng.argmax(self.C, axis=2)\n",
        "        self.C[:,:,:] = 0\n",
        "        for i in range(self.batch_size):\n",
        "            self.C[i, range(self.net_size), c[i, :]] = 1\n",
        "\n",
        "    def calculate_modularity(self):\n",
        "        QxC = self.Q_times_C(False)\n",
        "        q = eng.matmul(self.C.reshape((self.batch_size, 1, -1)), QxC.reshape((self.batch_size, -1, 1))).reshape((-1,))\n",
        "        return q\n",
        "\n",
        "    def activation(self,x): #ReLU\n",
        "        return x * (x>0)\n",
        "\n",
        "    def Q_times_C(self, strip_diagonal):\n",
        "        if self.use_sparse:\n",
        "            # Stack the vector batch into columns. (b, n, k) -> (n, b, k) -> (n, b*k)\n",
        "            C = eng.transpose(self.C, 0, 1).reshape((self.net_size, -1))\n",
        "            # And then reverse the reshaping. (n, n) x (n, b*k) = (n, b*k) -> (n, b, k) -> (b, n, k)\n",
        "            QxC = eng.transpose((self.sparse_Q @ C).reshape((self.net_size, self.batch_size, -1)), 1, 0)\n",
        "            QxC -= eng.matmul(self.w_out, eng.matmul(self.w_in, self.C))\n",
        "        else:\n",
        "            QxC = eng.matmul(self.Q, self.C)\n",
        "        if strip_diagonal:\n",
        "            return QxC - self.Q_diag * self.C\n",
        "        else:\n",
        "            return QxC\n",
        "\n",
        "    def calculate(self, params, init_partition, num_iterations, discretize_result=False):\n",
        "        self.batch_size = init_partition.shape[0] # number of random starting configurations\n",
        "        f0, f1, f2 = self.reshape_model_params(params)\n",
        "        # C.shape = batch_size x net_size x num_communities\n",
        "        self.C = init_partition\n",
        "        self.normalize_attachments()\n",
        "        for _ in range(num_iterations):\n",
        "            QxC = self.Q_times_C(self.strip_diagonal)\n",
        "            if self.normalize_QC: #normalize by max QxC\n",
        "                t = eng.abs(eng.max(QxC, axis=2, keepdims=True))\n",
        "                QxC /= t + (t == 0)\n",
        "            bias = eng.ones((self.batch_size, self.net_size, 1), dtype=float, device=\"cuda:0\")\n",
        "            next_C = f0 * bias + f1 * self.C + f2 * QxC\n",
        "            self.C = self.activation(next_C)\n",
        "            self.normalize_attachments()\n",
        "        if discretize_result:\n",
        "            self.discretize()\n",
        "        mod = self.calculate_modularity()\n",
        "        return self.C, mod\n",
        "\n",
        "    def normalize_attachments(self):\n",
        "        if self.normalize_each_step:\n",
        "            w = eng.sum(self.C, axis=2, keepdims=True)\n",
        "            self.C /= w + (w==0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tZp2RzmNiMcO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def runGNNSSeries(G, max_num_communities, iterations_per_stage, num_random_configs, fraction_to_keep,\n",
        "                discretize_last=True, init_params=None, init_communities=None, alpha=0.0, manual_gc=True, verbose=0):\n",
        "    num_model_params = 2\n",
        "    net_size = len(G)\n",
        "    max_batch_size = hypers.get('max_batch_size', 1000)\n",
        "    max_total_tensor_size = hypers.get('max_total_tensor_size', 200_000_000)\n",
        "    max_batch_size = max(1, min(max_batch_size, int(max_total_tensor_size / (net_size * max_num_communities))))\n",
        "    #print('net_size', net_size,\n",
        "    #      'max_num_communities =', max_num_communities,\n",
        "    #      'num_random_configs =', num_random_configs,\n",
        "    #      'max_batch_size =', max_batch_size,\n",
        "    #      'net_size * max_num_communities =',\n",
        "    #      net_size * max_num_communities)\n",
        "    start_time = time.time()\n",
        "    GNNS = GNNSModularityOptimizer(G, strip_diagonal=hypers.get('strip_diagonal', True),\n",
        "                                    normalize_modularity=hypers.get('normalize_modularity', False),\n",
        "                                    normalize_each_step=hypers.get('normalize_each_step', True),\n",
        "                                    use_sparse=hypers.get('use_sparse', False))\n",
        "    final_modularities = np.empty(0)\n",
        "    best_modularity = -1\n",
        "    # if num_random_configs is too large, do calculations in chunks\n",
        "    for i in range((num_random_configs + max_batch_size - 1) // max_batch_size):\n",
        "        cur_range = (i * max_batch_size, min((i + 1) * max_batch_size, num_random_configs))\n",
        "        batch_size = cur_range[1] - cur_range[0]\n",
        "        if init_params is None:\n",
        "            params = eng.random_uniform(low=0, high=1.0, size=(batch_size, num_model_params), device=\"cuda:0\")\n",
        "        else:\n",
        "            params = eng.tile(init_params.flatten(), (batch_size, 1))\n",
        "        if init_communities is None:\n",
        "            partition = eng.random_uniform(low=0, high=1.0, size=(batch_size, len(G), max_num_communities), device=\"cuda:0\")\n",
        "        else:\n",
        "            partition = eng.tile(init_communities, (batch_size, 1, 1))\n",
        "        for stage in range(len(iterations_per_stage)):\n",
        "            discretize = discretize_last and (stage == len(iterations_per_stage) - 1)\n",
        "            communities, modularities = GNNS.calculate(params, partition, iterations_per_stage[stage], discretize_result=discretize)\n",
        "            modularities = eng.numpy(modularities)\n",
        "            index_best = np.argmax(modularities)\n",
        "            if verbose > 0:\n",
        "                print('Stage {} completed'.format(stage+1))\n",
        "                print('Top modularity={}, mean={}, best_parameters={}'.format(modularities[index_best], np.mean(modularities), params[index_best]))\n",
        "            if stage < len(iterations_per_stage) - 1:\n",
        "                next_batch_size = max(1, batch_size * iterations_per_stage[0] // iterations_per_stage[stage+1])\n",
        "                selected_indices = modularities >= sorted(modularities)[-max(1, int(next_batch_size * fraction_to_keep))]\n",
        "                params = params[selected_indices, :]\n",
        "                # initialize with best partitions while keeping initial model params\n",
        "                partition = communities[selected_indices]\n",
        "                num_to_keep = sum(selected_indices)\n",
        "                num_to_repeat = next_batch_size - num_to_keep\n",
        "                if num_to_repeat > 0:\n",
        "                    # some king of soft-max; uniform for alpha=0\n",
        "                    weights = np.exp(modularities[selected_indices] * alpha)\n",
        "                    weights = weights / sum(weights)\n",
        "                    indices_to_repeat_partition = np.random.choice(num_to_keep, size=num_to_repeat, p=weights)\n",
        "                    repeated_partition = partition[indices_to_repeat_partition]\n",
        "                    partition = eng.concatenate([partition, repeated_partition], axis=0) # we copy initial partitions with best modularities\n",
        "                    # old way:\n",
        "                    #indices_to_repeat_params = np.random.choice(num_to_keep, size=num_to_repeat, p=weights)\n",
        "                    #repeated_params = params[indices_to_repeat_params, :num_model_params] # randomly permute repeated params\n",
        "                    #params = eng.concatenate([params, repeated_params], axis=0)\n",
        "                    new_params = eng.random_uniform(low=0, high=1.0, size=(num_to_repeat, num_model_params), device=\"cuda:0\")\n",
        "                    params = eng.concatenate([params, new_params], axis=0) # replace repeated params with random ones\n",
        "        final_modularities = np.concatenate([final_modularities, modularities])\n",
        "        if modularities[index_best] > best_modularity:\n",
        "            best_modularity = modularities[index_best]\n",
        "            best_communities = communities[index_best, :]\n",
        "            best_parameters = params[index_best]\n",
        "        if manual_gc:\n",
        "            del params, partition\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    if manual_gc:\n",
        "        del GNNS\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return best_communities, modularities, best_parameters, best_modularity, time.time()-start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vsmYewmrzQCo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def apply_method(arg):\n",
        "    G, weight_attr, method, seed = arg\n",
        "    modularity = -1\n",
        "    start_time = time.time()\n",
        "    if method == 'combo':\n",
        "        partition, modularity = pyCombo.execute(G, random_seed=seed)\n",
        "        partition_sets = {comm: set() for comm in partition.values()}\n",
        "        for node, comm in partition.items():\n",
        "            partition_sets[comm].add(node)\n",
        "        cdlib_comms = NodeClustering([list(c) for c in partition_sets.values()], G, \"Combo\", method_parameters={})\n",
        "    elif method == 'leiden':\n",
        "        partition = leidenalg.find_partition(G, leidenalg.ModularityVertexPartition, weights=weight_attr, n_iterations=-1, seed=seed)\n",
        "        cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"Leiden\", method_parameters={})\n",
        "    elif method == 'leiden_ig':\n",
        "        partition = G.community_leiden(weights=weight_attr, objective_function=\"modularity\", n_iterations=-1)\n",
        "        cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"Leiden_ig\", method_parameters={})\n",
        "    elif method == 'louvain':\n",
        "        cdlib_comms = algorithms.louvain(G)\n",
        "    elif method == 'louvain_ig':\n",
        "        partition = G.community_multilevel(weights=weight_attr)\n",
        "        cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"Luvain_ig\", method_parameters={})\n",
        "        modularity = partition.modularity\n",
        "    elif method == 'belief':\n",
        "        cdlib_comms = algorithms.belief(G)\n",
        "    elif method ==  'eigenvector':\n",
        "        partition = G.community_leading_eigenvector()\n",
        "        cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"Eigenvector\", method_parameters={})\n",
        "        modularity = partition.modularity\n",
        "    elif method ==  'greedy_modularity':\n",
        "        cdlib_comms = algorithms.greedy_modularity(G)\n",
        "    elif method ==  'greedy_modularity_ig':\n",
        "        partition = G.community_fastgreedy().as_clustering()\n",
        "        cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"CNM\", method_parameters={})\n",
        "        modularity = partition.modularity\n",
        "    elif method ==  'spinglass':\n",
        "        try:\n",
        "            partition = G.community_spinglass()\n",
        "            cdlib_comms = NodeClustering([G.vs[x][\"_nx_name\"] for x in partition], G, \"Spinglass\", method_parameters={})\n",
        "            modularity = partition.modularity\n",
        "        except:\n",
        "            cdlib_comms = NodeClustering([[0] * len(G.vs)], G, \"Spinglass\", method_parameters={})\n",
        "            modularity = 0\n",
        "    return cdlib_comms, modularity, time.time() - start_time\n",
        "\n",
        "def is_igraph_method(method):\n",
        "    return method == 'leiden' or method == 'leiden_ig' or method == 'louvain_ig' or method == 'eigenvector' or method == 'spinglass' or method == 'greedy_modularity_ig'\n",
        "\n",
        "def is_deterministic(method):\n",
        "    return method == 'greedy_modularity' or method == 'greedy_modularity_ig'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BRQCI1-viOVq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def partitionSeries(G, method, num_runs=10, verbose=0):\n",
        "    if num_runs > 0:\n",
        "        if is_deterministic(method):\n",
        "            num_runs = 1\n",
        "        partition = None\n",
        "        best_mod = -1\n",
        "        total_time = 0\n",
        "        results = eng.zeros((num_runs, 2)) # (mod, num_comms) pairs\n",
        "        graph = G\n",
        "        weight_attr = None\n",
        "        if is_igraph_method(method):\n",
        "            graph = iGraph.Graph.from_networkx(G)\n",
        "            if nx.is_weighted(G):\n",
        "                weight_attr = 'weight'\n",
        "        for i in range(num_runs):\n",
        "            cdlib_comms, modularity, elapsed_time = apply_method((graph, weight_attr, method, hypers['seed']+i))\n",
        "            if is_igraph_method(method):\n",
        "                cdlib_comms.graph = G\n",
        "            total_time += elapsed_time\n",
        "            if modularity == -1:\n",
        "                results[i, 0] = cdlib_comms.newman_girvan_modularity().score\n",
        "            else:\n",
        "                results[i, 0] = modularity\n",
        "            results[i, 1] = len(cdlib_comms.communities)\n",
        "            if best_mod < results[i, 0]:\n",
        "                best_mod = results[i, 0]\n",
        "                partition = cdlib_comms\n",
        "            if verbose > 1:\n",
        "                print(method + \"   mod_cdlib:\", results[i, 0])\n",
        "        max_mod_index = eng.argmax(results[:, 0])\n",
        "        if verbose > 0:\n",
        "            print('Best ' + method + ' modularity={}; {} communities'.format(results[max_mod_index, 0], results[max_mod_index, 1]))\n",
        "        res = {'best': results[max_mod_index, 0],\n",
        "               'best1': results[0, 0],\n",
        "               'best5': max(results[:5, 0]),\n",
        "               'best10': max(results[:10, 0]),\n",
        "               'best20': max(results[:20, 0]),\n",
        "               'min': results[:, 0].min(),\n",
        "               'mean': results[:, 0].mean(),\n",
        "               'comm_number': int(results[max_mod_index, 1]),\n",
        "               'partition': partition,\n",
        "               'total_time': total_time,\n",
        "               'avg_time': total_time / num_runs}\n",
        "    else:\n",
        "        res = {}\n",
        "    return res\n",
        "\n",
        "def partitionSeries_parallel(G, method, num_runs=10, verbose=0):\n",
        "    if num_runs > 0:\n",
        "        if is_deterministic(method):\n",
        "            num_runs = 1\n",
        "        partition = None\n",
        "        best_mod = -1\n",
        "        total_time = 0\n",
        "        results = eng.zeros((num_runs, 2)) # (mod, num_comms) pairs\n",
        "        graph = G\n",
        "        weight_attr = None\n",
        "        if is_igraph_method(method):\n",
        "            graph = iGraph.Graph.from_networkx(G)\n",
        "            if nx.is_weighted(G):\n",
        "                weight_attr = 'weight'\n",
        "        with Pool(hypers[\"num_processes\"]) as pool:\n",
        "            imap_unordered_it = pool.imap_unordered(apply_method,\n",
        "                                                    [(graph, weight_attr, method, hypers['seed']+i) for i in range(num_runs)],\n",
        "                                                    chunksize=hypers[\"num_processes\"])\n",
        "            i = 0\n",
        "            for res in imap_unordered_it:\n",
        "                cdlib_comms, modularity, elapsed_time = res\n",
        "                if is_igraph_method(method):\n",
        "                    cdlib_comms.graph = G\n",
        "                total_time += elapsed_time\n",
        "                if modularity == -1:\n",
        "                    results[i, 0] = cdlib_comms.newman_girvan_modularity().score\n",
        "                else:\n",
        "                    results[i, 0] = modularity\n",
        "                results[i, 1] = len(cdlib_comms.communities)\n",
        "                if best_mod < results[i, 0]:\n",
        "                    best_mod = results[i, 0]\n",
        "                    partition = cdlib_comms\n",
        "                if verbose > 1:\n",
        "                    print(method + \"   mod_cdlib:\", results[i, 0])\n",
        "                i += 1\n",
        "        max_mod_index = eng.argmax(results[:, 0])\n",
        "        if verbose > 0:\n",
        "            print('Best ' + method + ' modularity={}; {} communities'.format(results[max_mod_index, 0], results[max_mod_index, 1]))\n",
        "        res = {'best': results[max_mod_index, 0],\n",
        "               'best1': results[0, 0],\n",
        "               'best5': max(results[:5, 0]),\n",
        "               'best10': max(results[:10, 0]),\n",
        "               'best20': max(results[:20, 0]),\n",
        "               'min': results[:, 0].min(),\n",
        "               'mean': results[:, 0].mean(),\n",
        "               'comm_number': int(results[max_mod_index, 1]),\n",
        "               'partition': partition,\n",
        "               'total_time': total_time,\n",
        "               'avg_time': total_time / num_runs}\n",
        "    else:\n",
        "        res = {}\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vy8TrY6piRoN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def processNet(G, methods, num_runs=20, nums_initial_GNNS_configs=(100), verbose=0):\n",
        "    if num_runs > 0:\n",
        "        results = {}\n",
        "        comm_number = 0\n",
        "        best_mod = -1\n",
        "        for method in methods:\n",
        "            #results[method] = partitionSeries_parallel(G, num_runs=num_runs, method=method, verbose=verbose - 1)\n",
        "            results[method] = partitionSeries(G, num_runs=num_runs, method=method, verbose=verbose - 1)\n",
        "            if best_mod < results[method].get('best', 0):\n",
        "                best_mod = results[method].get('best', 0)\n",
        "                comm_number = max(comm_number, results[method].get('comm_number', 0))\n",
        "            if verbose > 0:\n",
        "                print(method + ' modularity best/best1/best5/best10/best20 = {:.6f}/{:.6f}/{:.6f}/{:.6f}/{:.6f}; min/avg = {:.6f}/{:.6f}; {} comm; time total/avg {:.2f}/{:.2f}'.format(\n",
        "                                    results[method].get('best', 0),\n",
        "                                    results[method].get('best1', 0),\n",
        "                                    results[method].get('best5', 0),\n",
        "                                    results[method].get('best10', 0),\n",
        "                                    results[method].get('best20', 0),\n",
        "                                    results[method].get('min', 0),\n",
        "                                    results[method].get('mean', 0),\n",
        "                                    results[method].get('comm_number', 0),\n",
        "                                    results[method].get('total_time', 0),\n",
        "                                    results[method].get('avg_time', 0)))\n",
        "        if comm_number <= 0:\n",
        "            comm_number = 50\n",
        "        comm_number = min(comm_number, 50)\n",
        "        GNN = {}\n",
        "        for num_initial_GNNS_configs in nums_initial_GNNS_configs:\n",
        "            set_all_random_seeds(hypers['seed'])\n",
        "            iterations_per_stage = [10, 10, 30]\n",
        "            if num_initial_GNNS_configs >= 1_000:\n",
        "                iterations_per_stage += [100]\n",
        "            if num_initial_GNNS_configs >= 10_000:\n",
        "                iterations_per_stage += [350]\n",
        "            fraction_to_keep = 1/3 #proportion of best configs to move to the next stage\n",
        "            C, _, best_params, best_mod, GNNS_time = runGNNSSeries(G, max_num_communities = comm_number + 1,\n",
        "                                                    iterations_per_stage=iterations_per_stage,\n",
        "                                                    num_random_configs=num_initial_GNNS_configs,\n",
        "                                                    fraction_to_keep=fraction_to_keep,\n",
        "                                                    manual_gc=(len(G) > 1000),\n",
        "                                                    verbose=verbose-1)\n",
        "            GNN[num_initial_GNNS_configs] = {'mod':best_mod,\n",
        "                                             'best_params':best_params,\n",
        "                                             'partition': C.argmax(axis=1),\n",
        "                                             'total_time': GNNS_time}\n",
        "            if verbose > 0:\n",
        "                print('GNN{} modularity = {}; best params = {}'.format(num_initial_GNNS_configs, GNN[num_initial_GNNS_configs]['mod'], GNN[num_initial_GNNS_configs]['best_params']))\n",
        "        #latex table output\n",
        "        print(' & '.join([G.name,\n",
        "                        ' & '.join(['%.6f' % results[method].get('best',0) for method in methods]),\n",
        "                        ' & '.join(['%.6f' % GNN[v].get('mod',0) for v in nums_initial_GNNS_configs]),\n",
        "                        ' & '.join(['%.2f' % results[method].get('total_time',0) for method in methods]),\n",
        "                        ' & '.join(['%.2f' % GNN[v].get('total_time',0) for v in nums_initial_GNNS_configs])\n",
        "                        ]))\n",
        "        return results, GNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YQioByakydsG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_graph(name, make_undirected=True, remove_weights=False, verbose=0):\n",
        "    if name[-4:] == \".net\":\n",
        "        G = nx.read_pajek(hypers['path_classic'] + name)\n",
        "        G.name = name[:-4]\n",
        "    elif name[-10:] == \".graph.bz2\":\n",
        "        with bz2.open(hypers['path_classic'] + name) as f:\n",
        "            G = nx.DiGraph()\n",
        "            i = 0\n",
        "            n = 0\n",
        "            m = 0\n",
        "            fmt = 0\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) == 0:\n",
        "                    if i > 0:\n",
        "                        i += 1\n",
        "                elif line[0] != '%':\n",
        "                    values = [int(x) for x in line.split()]\n",
        "                    if i == 0:\n",
        "                        n, m = values[:2]\n",
        "                        if len(values) > 2:\n",
        "                            fmt = values[2]\n",
        "                        if fmt > 1:\n",
        "                            print(\"Error in file: \", name, \" unsupported format!\")\n",
        "                            break\n",
        "                        G.add_nodes_from(range(n))\n",
        "                    else:\n",
        "                        if fmt == 0:\n",
        "                            G.add_edges_from([(i-1, j-1) for j in values])\n",
        "                        elif fmt == 1:\n",
        "                            G.add_weighted_edges_from([(i-1, j-1, w) for j, w in zip(values[::2], values[1::2])])\n",
        "                    i += 1\n",
        "        G.name = name[:-10]\n",
        "    num_selfloops = len(list(nx.selfloop_edges(G)))\n",
        "    if num_selfloops > 0:\n",
        "        print(name, \" self-loops: \", num_selfloops)\n",
        "    A = nx.to_scipy_sparse_matrix(G)\n",
        "    is_weighted = len(np.unique(A.data)) > 2 #there are values other than 0 and 1\n",
        "    is_directed = np.abs((A - A.transpose()).data).sum() > 1e-10\n",
        "    if remove_weights:\n",
        "        G = nx.from_scipy_sparse_matrix(A > 0)\n",
        "    if make_undirected or not is_directed:\n",
        "        G = nx.Graph(G)\n",
        "    if verbose > 0:\n",
        "        print('{} of size {}, with {} edges, directed = {}, weighted = {}, #self-loops = {}'.format(\n",
        "            name, len(G), G.number_of_edges(), is_directed, is_weighted, num_selfloops))\n",
        "    del A\n",
        "    gc.collect()\n",
        "    return G\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m444fVH6NdAR"
      },
      "outputs": [],
      "source": [
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "hypers = {}\n",
        "hypers['path_classic'] = './GNNS/networks/'\n",
        "\n",
        "#hypers['ENGINE'] = 'np'\n",
        "hypers['ENGINE'] = 'torch'\n",
        "eng = Engine(hypers['ENGINE'])\n",
        "\n",
        "hypers['seed'] = 13\n",
        "hypers['strip_diagonal'] = True\n",
        "hypers['normalize_modularity'] = False\n",
        "hypers['normalize_each_step'] = True\n",
        "hypers['use_sparse'] = True\n",
        "hypers['max_batch_size'] = 1000\n",
        "hypers['max_total_tensor_size'] = 100_000_000\n",
        "hypers[\"num_processes\"] = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bl1jcmRf9Ij",
        "outputId": "7a65b3c3-0a69-45da-bd19-59b55b2263b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network & leiden_mod & louvain_ig_mod & combo_mod & GNN100_mod & GNN2500_mod & leiden_time & louvain_ig_time & combo_time & GNN100_time & GNN2500_time\n",
            "karate & 0.419790 & 0.419790 & 0.419790 & 0.419790 & 0.419790 & 0.00 & 0.00 & 0.01 & 0.03 & 0.23\n",
            "chesapeake & 0.265796 & 0.265796 & 0.265796 & 0.265796 & 0.265796 & 0.01 & 0.00 & 0.01 & 0.03 & 0.22\n",
            "dolphins & 0.528519 & 0.524109 & 0.526799 & 0.528519 & 0.528519 & 0.01 & 0.00 & 0.01 & 0.03 & 0.24\n",
            "lesmis & 0.566688 & 0.566688 & 0.566688 & 0.566688 & 0.566688 & 0.01 & 0.00 & 0.03 & 0.03 & 0.26\n",
            "polbooks & 0.527237 & 0.526967 & 0.527237 & 0.527237 & 0.527237 & 0.04 & 0.02 & 0.29 & 0.11 & 0.48\n",
            "adjnoun & 0.309570 & 0.303934 & 0.310461 & 0.308758 & 0.310967 & 0.02 & 0.01 & 0.12 & 0.03 & 0.33\n",
            "football & 0.604570 & 0.604570 & 0.604570 & 0.602872 & 0.604570 & 0.01 & 0.00 & 0.08 & 0.03 & 0.34\n",
            "jazz & 0.445144 & 0.445144 & 0.444469 & 0.445144 & 0.445144 & 0.03 & 0.02 & 0.11 & 0.03 & 0.33\n",
            "celegansneural & 0.503485 & 0.498211 & 0.503782 & 0.502002 & 0.503736 & 0.04 & 0.02 & 0.56 & 0.03 & 0.47\n",
            "USAir97_undir & 0.212561 & 0.204418 & 0.214688 & 0.208066 & 0.213322 & 0.06 & 0.02 & 0.80 & 0.04 & 0.56\n",
            "celegans_metabolic & 0.452266 & 0.442862 & 0.453209 & 0.441670 & 0.446602 & 0.08 & 0.03 & 3.72 & 0.07 & 1.06\n",
            "email & 0.582497 & 0.577651 & 0.582792 & 0.568329 & 0.576863 & 0.28 & 0.08 & 28.09 & 0.90 & 3.98\n",
            "polblogs & 0.427105 & 0.427098 & 0.427096 & 0.426937 & 0.427059 & 0.23 & 0.11 & 15.12 & 1.35 & 17.02\n",
            "US Airports Network_undir & 0.275479 & 0.273658 & 0.275478 & 0.274653 & 0.275458 & 0.34 & 0.16 & 32.35 & 1.45 & 20.45\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_real_world_nets_small():\n",
        "    network_file_names = [\n",
        "        \"karate.graph.bz2\",\n",
        "        \"chesapeake.graph.bz2\",\n",
        "        \"dolphins.graph.bz2\",\n",
        "        \"lesmis.graph.bz2\",\n",
        "        \"polbooks.graph.bz2\", \n",
        "        \"adjnoun.graph.bz2\",\n",
        "        \"football.graph.bz2\",\n",
        "        \"jazz.graph.bz2\",\n",
        "        \"celegansneural.graph.bz2\",\n",
        "        \"USAir97_undir.net\",\n",
        "        \"celegans_metabolic.graph.bz2\",\n",
        "        \"email.graph.bz2\",\n",
        "        \"polblogs.graph.bz2\",\n",
        "        \"US Airports Network_undir.net\",\n",
        "    ]\n",
        "    return network_file_names\n",
        "\n",
        "def get_real_world_nets_big():\n",
        "    network_file_names = [\n",
        "        \"power.graph.bz2\",\n",
        "        \"PGPgiantcompo.graph.bz2\",\n",
        "        \"krong500slogn16.graph.bz2\",\n",
        "    ]\n",
        "    return network_file_names\n",
        "\n",
        "def run_real_world_nets(big_nets=False):\n",
        "    network_file_names = get_real_world_nets_small()\n",
        "    if big_nets:\n",
        "        network_file_names = get_real_world_nets_big()\n",
        "    methods = ['leiden', 'louvain_ig']#, 'spinglass', 'greedy_modularity_ig']#, 'eigenvector', 'belief']\n",
        "    if not big_nets:\n",
        "        methods += ['combo']\n",
        "    nums_initial_GNNS_configs = [100, 2500]\n",
        "    #latex table output\n",
        "    print(' & '.join(['Network', ' & '.join([method+'_mod' for method in methods]),\n",
        "                    ' & '.join(['GNN'+str(v)+'_mod' for v in nums_initial_GNNS_configs]),\n",
        "                    ' & '.join([method+'_time' for method in methods]),\n",
        "                    ' & '.join(['GNN'+str(v)+'_time' for v in nums_initial_GNNS_configs])]))\n",
        "    verbose = 0\n",
        "    for name in network_file_names:\n",
        "        if verbose > 0:\n",
        "            print(\"\\n\\n\\n\" + name)\n",
        "        G = read_graph(name, make_undirected=True)\n",
        "        processNet(G, methods, nums_initial_GNNS_configs=nums_initial_GNNS_configs, num_runs=10, verbose=verbose)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "run_real_world_nets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1b0kj_6NNzb",
        "outputId": "7be75d8a-8077-4a42-fc56-9daa7bd8e995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network & leiden_mod & louvain_ig_mod & GNN100_mod & GNN2500_mod & leiden_time & louvain_ig_time & GNN100_time & GNN2500_time\n",
            "power & 0.940714 & 0.936614 & 0.818490 & 0.880699 & 6.28 & 1.12 & 1.43 & 39.60\n",
            "PGPgiantcompo & 0.886640 & 0.884784 & 0.838746 & 0.865027 & 11.41 & 2.59 & 3.32 & 101.95\n",
            "krong500slogn16 & 0.063362 & 0.059964 & 0.058143 & 0.064676 & 5983.00 & 298.22 & 45.78 & 1061.30\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "run_real_world_nets(True)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmZblWrLNPhH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_BM_series(num_nets, block_sizes, inner_prob_mult_factor):\n",
        "    # inner_prob_mult_factor - how much stronger are the inner edges\n",
        "    block_sizes = np.array(block_sizes) # sizes of the blocks\n",
        "    average_prob = 0.1 # average probability of connections\n",
        "    net_size = sum(block_sizes)\n",
        "    num_blocks = len(block_sizes)\n",
        "    Gs = [0] * num_nets\n",
        "    selfloops = False\n",
        "    for i in range(num_nets):\n",
        "        num_inner_edges = (block_sizes * (block_sizes - 1 + selfloops)).sum()\n",
        "        num_total_edges = net_size * (net_size - 1 + selfloops)\n",
        "        prob_of_inner_edge = num_inner_edges / num_total_edges\n",
        "        # average_prob = outer_prob*(1-prob_of_inner_edge) + inner_prob_mult_factor*outer_prob*prob_of_inner_edge =>\n",
        "        outer_prob = average_prob / ((inner_prob_mult_factor - 1) * prob_of_inner_edge + 1) #probability between groups\n",
        "        P = outer_prob * (np.ones((num_blocks, num_blocks)) + np.eye(num_blocks) * (inner_prob_mult_factor - 1))\n",
        "        communities = {i: sum(np.cumsum(block_sizes) <= i) for i in range(net_size)}\n",
        "        Gs[i] = (nx.stochastic_block_model(block_sizes, P, selfloops=selfloops), communities)\n",
        "    return Gs\n",
        "\n",
        "def generate_LFR_series(num_nets=10, net_size=250, mu=0.1, min_degree=20, max_degree=50, min_community=20):\n",
        "    hypers['LFR_tau1']=2.0\n",
        "    hypers['LFR_tau2']=1.1\n",
        "    Gs = [0] * num_nets\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < num_nets:\n",
        "        try:\n",
        "            G = nx.LFR_benchmark_graph(net_size, tau1=hypers['LFR_tau1'], tau2=hypers['LFR_tau2'], mu=mu,\n",
        "                                        min_degree=min_degree, max_degree=max_degree, min_community=min_community, seed=hypers['seed']+j)\n",
        "            G.name = 'LFR' + str(i)\n",
        "            comms = {}\n",
        "            for v in G.nodes:\n",
        "                if v in comms:\n",
        "                    label = comms[v]\n",
        "                else:\n",
        "                    label = len(np.unique(list(comms.values())))\n",
        "                    for u in G.nodes[v]['community']:\n",
        "                        comms[u] = label\n",
        "            Gs[i] = (G, comms)\n",
        "            i += 1\n",
        "        except:\n",
        "            pass\n",
        "        j += 1\n",
        "    return Gs\n",
        "\n",
        "def evalSeries(num_nets=10, net_size=150, netType='BM', num_initial_GNNS_configs=100, num_runs=1,\n",
        "                bm_inner_prob_mult_factor=1.5, lfr_mu=0.35, lfr_min_degree=20, lfr_max_degree=50, lfr_min_community=20):\n",
        "    def NMI(part1, part2): #NMI between two partition dictionaries\n",
        "        n = len(part1)\n",
        "        p2 = [0] * n\n",
        "        if type(part2) is list:\n",
        "            for label, comm in enumerate(part2):\n",
        "                for v in comm:\n",
        "                    p2[v] = label\n",
        "        else:\n",
        "            for v, comm in enumerate(part2):\n",
        "                p2[v] = eng.numpy(comm)\n",
        "        p1 = [part1[p] for p in range(n)]\n",
        "        return normalized_mutual_info_score(p1, p2)\n",
        "\n",
        "    seed = hypers['seed']\n",
        "    set_all_random_seeds(seed)\n",
        "    if netType == 'BM':\n",
        "        G = generate_BM_series(num_nets, (net_size//3, net_size//3, net_size//3), bm_inner_prob_mult_factor)\n",
        "    else:\n",
        "        G = generate_LFR_series(num_nets, net_size, lfr_mu, lfr_min_degree, lfr_max_degree, lfr_min_community)\n",
        "        num_nets = len(G)\n",
        "    methods = ['leiden', 'louvain_ig', 'combo']\n",
        "    num_methods = len(methods) + 1\n",
        "    mods = np.zeros((num_nets, num_methods))\n",
        "    nmi = np.zeros((num_nets, num_methods))\n",
        "    elapsedTime = [0] * num_methods\n",
        "    total_avg_degree = 0\n",
        "    for i in range(num_nets):\n",
        "        avg_degree = sum(list(zip(*list(G[i][0].degree())))[1]) / len(G[i][0])\n",
        "        print('Network {}/{}, size = {}, avg. degree = {}'.format(i+1, num_nets, len(G[i][0]), avg_degree))\n",
        "        res, resGNN = processNet(G[i][0], methods, num_runs, [num_initial_GNNS_configs], verbose=0)\n",
        "        mods[i, :] = [res[method]['best'] for method in methods] + [resGNN[num_initial_GNNS_configs]['mod']]\n",
        "        nmi[i,:] = [NMI(G[i][1], res[method]['partition'].communities) for method in methods] + [NMI(G[i][1], resGNN[num_initial_GNNS_configs]['partition'])]\n",
        "        for j, method in enumerate(methods):\n",
        "            elapsedTime[j] += res[method]['total_time'] / num_nets\n",
        "        elapsedTime[3] += resGNN[num_initial_GNNS_configs]['total_time'] / num_nets\n",
        "        total_avg_degree += avg_degree / num_nets\n",
        "    print(\"average degree = \", total_avg_degree)\n",
        "    print('methods = %s & %s & %s & %s'%tuple(methods + ['GNNS'+str(num_initial_GNNS_configs)]))\n",
        "    print('Mod = %.6f & %.6f & %.6f & %.6f'%tuple(mods.mean(axis=0)))\n",
        "    print('NMI = %.6f & %.6f & %.6f & %.6f'%tuple(nmi.mean(axis=0)))\n",
        "    print('Time = %.3f & %.3f & %.3f & %.3f'%tuple(elapsedTime))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os-1SbhRNREX",
        "outputId": "9196e0ff-5953-451b-cf95-bd955b3d9a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network 1/10, size = 250, avg. degree = 29.768\n",
            "LFR0 & 0.227704 & 0.227139 & 0.227455 & 0.227575 & 0.50 & 0.29 & 5.64 & 0.04\n",
            "Network 2/10, size = 250, avg. degree = 28.432\n",
            "LFR1 & 0.233267 & 0.232769 & 0.233812 & 0.233420 & 0.50 & 0.24 & 7.14 & 0.05\n",
            "Network 3/10, size = 250, avg. degree = 30.2\n",
            "LFR2 & 0.221316 & 0.220588 & 0.221756 & 0.220485 & 0.61 & 0.28 & 5.76 & 0.04\n",
            "Network 4/10, size = 250, avg. degree = 29.984\n",
            "LFR3 & 0.221645 & 0.218156 & 0.221137 & 0.221001 & 0.60 & 0.26 & 6.42 & 0.04\n",
            "Network 5/10, size = 250, avg. degree = 30.28\n",
            "LFR4 & 0.216868 & 0.215753 & 0.217309 & 0.217130 & 0.58 & 0.34 & 9.06 & 0.05\n",
            "Network 6/10, size = 250, avg. degree = 29.536\n",
            "LFR5 & 0.238262 & 0.237164 & 0.238262 & 0.238180 & 0.43 & 0.25 & 5.74 & 0.05\n",
            "Network 7/10, size = 250, avg. degree = 29.344\n",
            "LFR6 & 0.243997 & 0.243267 & 0.244261 & 0.243424 & 0.43 & 0.23 & 5.75 & 0.05\n",
            "Network 8/10, size = 250, avg. degree = 29.376\n",
            "LFR7 & 0.242806 & 0.242207 & 0.242889 & 0.242216 & 0.52 & 0.24 & 5.10 & 0.04\n",
            "Network 9/10, size = 250, avg. degree = 28.944\n",
            "LFR8 & 0.237262 & 0.236209 & 0.237541 & 0.237107 & 0.66 & 0.25 & 5.98 & 0.04\n",
            "Network 10/10, size = 250, avg. degree = 29.552\n",
            "LFR9 & 0.233256 & 0.231778 & 0.233850 & 0.232952 & 0.51 & 0.30 & 6.52 & 0.04\n",
            "average degree =  29.541600000000003\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.231638 & 0.230503 & 0.231827 & 0.231349\n",
            "NMI = 0.651277 & 0.641316 & 0.631941 & 0.652583\n",
            "Time = 0.534 & 0.268 & 6.309 & 0.045\n",
            "Network 1/10, size = 250, avg. degree = 13.8\n",
            "LFR0 & 0.317542 & 0.310836 & 0.318135 & 0.316378 & 0.53 & 0.14 & 6.59 & 0.04\n",
            "Network 2/10, size = 250, avg. degree = 13.224\n",
            "LFR1 & 0.319530 & 0.308412 & 0.319520 & 0.312030 & 0.51 & 0.13 & 6.26 & 0.04\n",
            "Network 3/10, size = 250, avg. degree = 13.272\n",
            "LFR2 & 0.321201 & 0.317747 & 0.323715 & 0.318175 & 0.46 & 0.15 & 5.24 & 0.04\n",
            "Network 4/10, size = 250, avg. degree = 13.0\n",
            "LFR3 & 0.322112 & 0.319331 & 0.322180 & 0.317998 & 0.46 & 0.15 & 5.57 & 0.04\n",
            "Network 5/10, size = 250, avg. degree = 13.304\n",
            "LFR4 & 0.318216 & 0.311171 & 0.320072 & 0.315255 & 0.50 & 0.13 & 6.11 & 0.05\n",
            "Network 6/10, size = 250, avg. degree = 13.384\n",
            "LFR5 & 0.312577 & 0.310013 & 0.312907 & 0.314077 & 0.52 & 0.13 & 7.27 & 0.04\n",
            "Network 7/10, size = 250, avg. degree = 12.784\n",
            "LFR6 & 0.322165 & 0.317961 & 0.320391 & 0.316074 & 0.56 & 0.15 & 6.78 & 0.04\n",
            "Network 8/10, size = 250, avg. degree = 12.128\n",
            "LFR7 & 0.331330 & 0.322828 & 0.333413 & 0.329093 & 0.54 & 0.14 & 6.15 & 0.04\n",
            "Network 9/10, size = 250, avg. degree = 13.336\n",
            "LFR8 & 0.334405 & 0.332287 & 0.335268 & 0.331534 & 0.48 & 0.14 & 6.04 & 0.04\n",
            "Network 10/10, size = 250, avg. degree = 13.792\n",
            "LFR9 & 0.334267 & 0.329557 & 0.334332 & 0.331457 & 0.41 & 0.14 & 5.99 & 0.04\n",
            "average degree =  13.2024\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.323335 & 0.318014 & 0.323993 & 0.320207\n",
            "NMI = 0.557155 & 0.544109 & 0.511360 & 0.536949\n",
            "Time = 0.498 & 0.140 & 6.200 & 0.043\n",
            "Network 1/10, size = 300, avg. degree = 30.30666666666667\n",
            "stochastic_block_model & 0.160633 & 0.151685 & 0.165825 & 0.160341 & 0.96 & 0.44 & 8.33 & 0.05\n",
            "Network 2/10, size = 300, avg. degree = 29.58\n",
            "stochastic_block_model & 0.161285 & 0.154234 & 0.166234 & 0.163267 & 1.04 & 0.46 & 7.24 & 0.05\n",
            "Network 3/10, size = 300, avg. degree = 29.893333333333334\n",
            "stochastic_block_model & 0.160535 & 0.151615 & 0.166258 & 0.159096 & 0.96 & 0.42 & 7.72 & 0.05\n",
            "Network 4/10, size = 300, avg. degree = 30.273333333333333\n",
            "stochastic_block_model & 0.158018 & 0.153808 & 0.166191 & 0.158438 & 0.80 & 0.51 & 6.33 & 0.05\n",
            "Network 5/10, size = 300, avg. degree = 30.453333333333333\n",
            "stochastic_block_model & 0.158084 & 0.151347 & 0.165504 & 0.163108 & 0.77 & 0.48 & 8.02 & 0.05\n",
            "Network 6/10, size = 300, avg. degree = 30.326666666666668\n",
            "stochastic_block_model & 0.156876 & 0.148005 & 0.163706 & 0.153893 & 0.95 & 0.41 & 6.81 & 0.05\n",
            "Network 7/10, size = 300, avg. degree = 30.28\n",
            "stochastic_block_model & 0.158208 & 0.152707 & 0.165071 & 0.157946 & 0.89 & 0.47 & 8.16 & 0.05\n",
            "Network 8/10, size = 300, avg. degree = 29.74\n",
            "stochastic_block_model & 0.158857 & 0.155195 & 0.168372 & 0.160321 & 0.86 & 0.46 & 8.33 & 0.05\n",
            "Network 9/10, size = 300, avg. degree = 30.326666666666668\n",
            "stochastic_block_model & 0.162329 & 0.155485 & 0.164400 & 0.162916 & 0.84 & 0.44 & 7.30 & 0.05\n",
            "Network 10/10, size = 300, avg. degree = 29.62\n",
            "stochastic_block_model & 0.161106 & 0.149568 & 0.166053 & 0.158932 & 0.93 & 0.41 & 7.56 & 0.05\n",
            "average degree =  30.080000000000002\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.159593 & 0.152365 & 0.165762 & 0.159826\n",
            "NMI = 0.050049 & 0.049271 & 0.036201 & 0.052641\n",
            "Time = 0.902 & 0.448 & 7.580 & 0.051\n",
            "Network 1/10, size = 300, avg. degree = 30.293333333333333\n",
            "stochastic_block_model & 0.175501 & 0.167503 & 0.179987 & 0.174267 & 0.95 & 0.50 & 7.18 & 0.04\n",
            "Network 2/10, size = 300, avg. degree = 30.386666666666667\n",
            "stochastic_block_model & 0.175347 & 0.163779 & 0.181243 & 0.172628 & 0.90 & 0.44 & 10.45 & 0.20\n",
            "Network 3/10, size = 300, avg. degree = 29.613333333333333\n",
            "stochastic_block_model & 0.171848 & 0.165765 & 0.179738 & 0.176984 & 1.22 & 0.65 & 8.78 & 0.05\n",
            "Network 4/10, size = 300, avg. degree = 29.926666666666666\n",
            "stochastic_block_model & 0.177265 & 0.161864 & 0.182706 & 0.178856 & 0.82 & 0.44 & 8.57 & 0.04\n",
            "Network 5/10, size = 300, avg. degree = 30.92\n",
            "stochastic_block_model & 0.170772 & 0.166365 & 0.177748 & 0.173781 & 0.93 & 0.44 & 6.15 & 0.04\n",
            "Network 6/10, size = 300, avg. degree = 30.293333333333333\n",
            "stochastic_block_model & 0.176701 & 0.165321 & 0.183713 & 0.175776 & 0.94 & 0.42 & 6.79 & 0.05\n",
            "Network 7/10, size = 300, avg. degree = 29.926666666666666\n",
            "stochastic_block_model & 0.177356 & 0.164782 & 0.183047 & 0.177159 & 1.16 & 0.46 & 7.20 & 0.04\n",
            "Network 8/10, size = 300, avg. degree = 29.013333333333332\n",
            "stochastic_block_model & 0.168115 & 0.160942 & 0.176263 & 0.165540 & 0.92 & 0.41 & 8.17 & 0.05\n",
            "Network 9/10, size = 300, avg. degree = 29.69333333333333\n",
            "stochastic_block_model & 0.184605 & 0.170962 & 0.186397 & 0.179581 & 0.92 & 0.49 & 8.87 & 0.04\n",
            "Network 10/10, size = 300, avg. degree = 30.113333333333333\n",
            "stochastic_block_model & 0.174829 & 0.161699 & 0.176751 & 0.172984 & 0.81 & 0.54 & 6.26 & 0.04\n",
            "average degree =  30.018\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.175234 & 0.164898 & 0.180759 & 0.174756\n",
            "NMI = 0.335723 & 0.275103 & 0.456881 & 0.322522\n",
            "Time = 0.958 & 0.480 & 7.842 & 0.060\n",
            "Network 1/10, size = 300, avg. degree = 30.493333333333332\n",
            "stochastic_block_model & 0.222145 & 0.221348 & 0.222145 & 0.222145 & 0.62 & 0.38 & 2.68 & 0.05\n",
            "Network 2/10, size = 300, avg. degree = 30.48\n",
            "stochastic_block_model & 0.227769 & 0.227174 & 0.227769 & 0.227769 & 0.51 & 0.47 & 2.76 & 0.04\n",
            "Network 3/10, size = 300, avg. degree = 29.48\n",
            "stochastic_block_model & 0.216092 & 0.214071 & 0.216092 & 0.216092 & 0.59 & 0.40 & 2.91 & 0.04\n",
            "Network 4/10, size = 300, avg. degree = 30.30666666666667\n",
            "stochastic_block_model & 0.224442 & 0.223441 & 0.224442 & 0.224442 & 0.52 & 0.48 & 2.02 & 0.04\n",
            "Network 5/10, size = 300, avg. degree = 30.113333333333333\n",
            "stochastic_block_model & 0.231359 & 0.231315 & 0.231359 & 0.231359 & 0.48 & 0.38 & 1.85 & 0.04\n",
            "Network 6/10, size = 300, avg. degree = 30.02\n",
            "stochastic_block_model & 0.227025 & 0.225573 & 0.227025 & 0.226877 & 0.53 & 0.39 & 1.48 & 0.04\n",
            "Network 7/10, size = 300, avg. degree = 29.846666666666668\n",
            "stochastic_block_model & 0.227388 & 0.226519 & 0.227388 & 0.227388 & 0.50 & 0.42 & 2.67 & 0.05\n",
            "Network 8/10, size = 300, avg. degree = 29.046666666666667\n",
            "stochastic_block_model & 0.221327 & 0.219049 & 0.221327 & 0.221327 & 0.65 & 0.53 & 2.96 & 0.04\n",
            "Network 9/10, size = 300, avg. degree = 29.873333333333335\n",
            "stochastic_block_model & 0.221855 & 0.221239 & 0.221855 & 0.221855 & 0.58 & 0.45 & 1.90 & 0.04\n",
            "Network 10/10, size = 300, avg. degree = 29.126666666666665\n",
            "stochastic_block_model & 0.225829 & 0.224851 & 0.225829 & 0.225827 & 0.55 & 0.43 & 2.19 & 0.04\n",
            "average degree =  29.878666666666664\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.224523 & 0.223458 & 0.224523 & 0.224508\n",
            "NMI = 0.877335 & 0.859546 & 0.877335 & 0.876657\n",
            "Time = 0.552 & 0.434 & 2.342 & 0.041\n",
            "Network 1/10, size = 300, avg. degree = 30.48\n",
            "stochastic_block_model & 0.262642 & 0.262642 & 0.262642 & 0.262642 & 0.46 & 0.28 & 1.79 & 0.04\n",
            "Network 2/10, size = 300, avg. degree = 30.4\n",
            "stochastic_block_model & 0.265691 & 0.265691 & 0.265691 & 0.265691 & 0.41 & 0.27 & 1.50 & 0.04\n",
            "Network 3/10, size = 300, avg. degree = 29.526666666666667\n",
            "stochastic_block_model & 0.261965 & 0.261965 & 0.261965 & 0.261965 & 0.55 & 0.26 & 1.42 & 0.04\n",
            "Network 4/10, size = 300, avg. degree = 29.7\n",
            "stochastic_block_model & 0.267916 & 0.267916 & 0.267916 & 0.267916 & 0.39 & 0.31 & 1.34 & 0.04\n",
            "Network 5/10, size = 300, avg. degree = 29.866666666666667\n",
            "stochastic_block_model & 0.269139 & 0.269139 & 0.269139 & 0.269139 & 0.35 & 0.28 & 1.26 & 0.04\n",
            "Network 6/10, size = 300, avg. degree = 30.273333333333333\n",
            "stochastic_block_model & 0.264102 & 0.264102 & 0.264102 & 0.264102 & 0.43 & 0.29 & 1.99 & 0.04\n",
            "Network 7/10, size = 300, avg. degree = 30.073333333333334\n",
            "stochastic_block_model & 0.266181 & 0.266181 & 0.266181 & 0.266181 & 0.35 & 0.28 & 1.52 & 0.04\n",
            "Network 8/10, size = 300, avg. degree = 29.80666666666667\n",
            "stochastic_block_model & 0.256422 & 0.256422 & 0.256422 & 0.256422 & 0.57 & 0.33 & 1.40 & 0.04\n",
            "Network 9/10, size = 300, avg. degree = 29.94\n",
            "stochastic_block_model & 0.260304 & 0.260304 & 0.260304 & 0.260304 & 0.49 & 0.26 & 2.50 & 0.13\n",
            "Network 10/10, size = 300, avg. degree = 29.44\n",
            "stochastic_block_model & 0.261718 & 0.261718 & 0.261718 & 0.261718 & 0.46 & 0.37 & 1.82 & 0.04\n",
            "average degree =  29.950666666666663\n",
            "methods = leiden & louvain_ig & combo & GNNS100\n",
            "Mod = 0.263608 & 0.263608 & 0.263608 & 0.263608\n",
            "NMI = 0.972523 & 0.972523 & 0.972523 & 0.972523\n",
            "Time = 0.446 & 0.293 & 1.655 & 0.050\n"
          ]
        }
      ],
      "source": [
        "def run_synthetic_nets():\n",
        "    evalSeries(num_nets=10, net_size=250, netType = 'LFR', num_initial_GNNS_configs=100, num_runs=20,\n",
        "            lfr_mu=0.45, lfr_min_degree=15, lfr_max_degree=40, lfr_min_community=20)\n",
        "    evalSeries(num_nets=10, net_size=250, netType = 'LFR', num_initial_GNNS_configs=100, num_runs=20,\n",
        "            lfr_mu=0.45, lfr_min_degree=5, lfr_max_degree=30, lfr_min_community=20)\n",
        "    for factor in (1.5, 2.0, 2.5, 3.0):\n",
        "        evalSeries(num_nets=10, net_size=300, netType = 'BM', num_initial_GNNS_configs=100, num_runs=20,\n",
        "                   bm_inner_prob_mult_factor=factor)\n",
        "\n",
        "run_synthetic_nets()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GNNS.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aa8c9e988bba47ec3e791b22c0bbb49eb66a938b6cde8fffd564472f09fd563a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
